# main.py — KRO v30.2 — чистая рабочая версия (Render-ready)

import os
import json
import numpy as np
from datetime import datetime, timedelta
from fastapi import FastAPI
import uvicorn
from sentence_transformers import SentenceTransformer
from groq import Groq
import gradio as gr
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from langchain_huggingface import HuggingFaceEmbeddings

# Твой ключ (не коммить в GitHub!)
GROQ_API_KEY = "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"

groq_client = Groq(api_key=GROQ_API_KEY)

def llm_generate(prompt: str, max_tokens: int = 400, temperature: float = 0.7) -> str:
    try:
        resp = groq_client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model="llama-3.1-70b-versatile",
            max_tokens=max_tokens,
            temperature=temperature
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        print(f"Groq error: {e}")
        return "KRO: LLM недоступен."

DEVICE = "cpu"  # упрощаем, Render обычно без GPU
print(f"Device: {DEVICE}")

DATA_DIR = "/data"
CONVERSATION_COLLECTION = "kro_conversation"
ARCHIVE_DIR = f"{DATA_DIR}/conversation_archive"
LAST_COMPRESS_PATH = f"{DATA_DIR}/last_compress.json"

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(ARCHIVE_DIR, exist_ok=True)

# Embedding + Chroma
embedder = SentenceTransformer("all-MiniLM-L6-v2", device=DEVICE)
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

vectorstore = Chroma(
    collection_name=CONVERSATION_COLLECTION,
    embedding_function=embeddings,
    persist_directory=f"{DATA_DIR}/chroma_db"
)

class KRO:
    def __init__(self):
        self.step_id = 0
        self.last_compress_time = datetime.now() - timedelta(days=8)

        if os.path.exists(LAST_COMPRESS_PATH):
            try:
                with open(LAST_COMPRESS_PATH, "r") as f:
                    info = json.load(f)
                    self.last_compress_time = datetime.fromisoformat(info["last_compress"])
            except:
                pass

    def should_compress(self):
        return (datetime.now() - self.last_compress_time).days >= 7

    def compress_conversation(self):
        cutoff = datetime.now() - timedelta(days=7)
        results = vectorstore.get(where={"timestamp": {"$lt": cutoff.isoformat()}})
        old_docs = results.get("documents", [])
        if len(old_docs) < 50:
            return

        old_text = "\n".join(old_docs[-300:])
        if len(old_text) > 30000:
            old_text = old_text[:30000]

        prompt = f"""Суммаризируй эту историю разговора в 400–600 слов.
Выдели ключевые темы, эмоции, важные события.
Сохрани суть взаимодействия.

Сообщения:
{old_text}"""

        summary = llm_generate(prompt, max_tokens=800)

        month_key = datetime.now().strftime("%Y-%m")
        archive_path = f"{ARCHIVE_DIR}/archive_{month_key}.json"
        archive = []
        if os.path.exists(archive_path):
            try:
                with open(archive_path, "r", encoding="utf-8") as f:
                    archive = json.load(f)
            except:
                pass
        archive.append({
            "period": f"{results['metadatas'][0]['timestamp']} → {results['metadatas'][-1]['timestamp']}",
            "summary": summary,
            "entry_count": len(old_docs)
        })
        with open(archive_path, "w", encoding="utf-8") as f:
            json.dump(archive, f, ensure_ascii=False, indent=2)

        print(f"[Сжатие] {len(old_docs)} сообщений → {archive_path}")

        ids_to_delete = results.get("ids", [])
        if ids_to_delete:
            vectorstore.delete(ids=ids_to_delete)

        summary_doc = Document(
            page_content=summary,
            metadata={"type": "summary", "timestamp": datetime.now().isoformat()}
        )
        vectorstore.add_documents([summary_doc])

        self.last_compress_time = datetime.now()
        with open(LAST_COMPRESS_PATH, "w") as f:
            json.dump({"last_compress": self.last_compress_time.isoformat()}, f)

    def add_to_history(self, user_text: str, kro_reply: str):
        timestamp = datetime.now().isoformat()
        doc_user = Document(
            page_content=user_text,
            metadata={"role": "user", "timestamp": timestamp}
        )
        doc_kro = Document(
            page_content=kro_reply,
            metadata={"role": "kro", "timestamp": timestamp}
        )
        vectorstore.add_documents([doc_user, doc_kro])

    def search_history(self, query: str, k=5):
        try:
            results = vectorstore.similarity_search(query, k=k)
            return [doc.page_content for doc in results]
        except:
            return []

    def step(self, text: str):
        self.step_id += 1

        relevant = self.search_history(text, k=5)
        context = "\n".join(relevant)

        prompt = f"""Контекст из памяти:
{context}

Пользователь: {text}

Ответь как KRO."""
        reply = llm_generate(prompt)

        self.add_to_history(text, reply)

        if self.should_compress():
            self.compress_conversation()

        return reply

kro = KRO()

# ============================================================
# FastAPI + Gradio
# ============================================================

from fastapi import FastAPI
import gradio as gr
import os
import uvicorn

app = FastAPI(title="KRO v30 API")

@app.get("/")
def root():
    return {"status": "KRO v30 alive"}

# создаём интерфейс
gradio_interface = gr.Interface(
    fn=lambda message, history: (history + [(message, kro.step(message))], ""),
    inputs=[gr.Textbox(label="Сообщение"), gr.Chatbot()],
    outputs=[gr.Chatbot(), gr.Textbox(label="Ответ")],
    title="KRO v30 — Постоянная память + Chroma"
)

# ВАЖНО: mount на /ui
app = gr.mount_gradio_app(app, gradio_interface, path="/ui")

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 10000))
    print(f"Запуск KRO v30 на порту {port}")
    uvicorn.run(app, host="0.0.0.0", port=port)
