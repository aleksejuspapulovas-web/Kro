# ============================================================
# KRO v30.2 — Render Ready Production Version
# ============================================================

import os
import json
from datetime import datetime, timedelta

from fastapi import FastAPI
import uvicorn
import gradio as gr

from groq import Groq
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from langchain_huggingface import HuggingFaceEmbeddings

# ============================================================
# Environment
# ============================================================

GROQ_API_KEY = os.environ.get("GROQ_API_KEY")

if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEY not found in environment variables")

groq_client = Groq(api_key=GROQ_API_KEY)

DATA_DIR = "./data"
ARCHIVE_DIR = f"{DATA_DIR}/conversation_archive"
LAST_COMPRESS_PATH = f"{DATA_DIR}/last_compress.json"
CHROMA_DIR = f"{DATA_DIR}/chroma_db"

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(ARCHIVE_DIR, exist_ok=True)

# ============================================================
# Embeddings + Vector Store
# ============================================================

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

vectorstore = Chroma(
    collection_name="kro_conversation",
    embedding_function=embeddings,
    persist_directory=CHROMA_DIR
)

# ============================================================
# LLM Helper
# ============================================================

def llm_generate(prompt: str, max_tokens: int = 400, temperature: float = 0.7) -> str:
    try:
        resp = groq_client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model="llama-3.1-70b-versatile",
            max_tokens=max_tokens,
            temperature=temperature
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        print(f"[Groq error]: {e}")
        return "KRO: LLM недоступен."

# ============================================================
# Core KRO Logic
# ============================================================

class KRO:
    def __init__(self):
        self.last_compress_time = datetime.now() - timedelta(days=8)

        if os.path.exists(LAST_COMPRESS_PATH):
            try:
                with open(LAST_COMPRESS_PATH, "r") as f:
                    info = json.load(f)
                    self.last_compress_time = datetime.fromisoformat(info["last_compress"])
            except:
                pass

    def should_compress(self):
        return (datetime.now() - self.last_compress_time).days >= 7

    def compress_conversation(self):
        cutoff = datetime.now() - timedelta(days=7)

        try:
            results = vectorstore.get()
        except:
            return

        documents = results.get("documents", [])
        metadatas = results.get("metadatas", [])
        ids = results.get("ids", [])

        old_docs = []
        old_ids = []

        for doc, meta, doc_id in zip(documents, metadatas, ids):
            if meta and "timestamp" in meta:
                if meta["timestamp"] < cutoff.isoformat():
                    old_docs.append(doc)
                    old_ids.append(doc_id)

        if len(old_docs) < 50:
            return

        old_text = "\n".join(old_docs[-300:])
        old_text = old_text[:30000]

        prompt = f"""
Суммаризируй историю разговора (400–600 слов).
Выдели ключевые темы, эмоции и важные события.

Сообщения:
{old_text}
"""

        summary = llm_generate(prompt, max_tokens=800)

        month_key = datetime.now().strftime("%Y-%m")
        archive_path = f"{ARCHIVE_DIR}/archive_{month_key}.json"

        archive = []
        if os.path.exists(archive_path):
            try:
                with open(archive_path, "r", encoding="utf-8") as f:
                    archive = json.load(f)
            except:
                pass

        archive.append({
            "period": "auto-compressed",
            "summary": summary,
            "entry_count": len(old_docs)
        })

        with open(archive_path, "w", encoding="utf-8") as f:
            json.dump(archive, f, ensure_ascii=False, indent=2)

        if old_ids:
            vectorstore.delete(ids=old_ids)

        summary_doc = Document(
            page_content=summary,
            metadata={"type": "summary", "timestamp": datetime.now().isoformat()}
        )

        vectorstore.add_documents([summary_doc])
        vectorstore.persist()

        self.last_compress_time = datetime.now()

        with open(LAST_COMPRESS_PATH, "w") as f:
            json.dump({"last_compress": self.last_compress_time.isoformat()}, f)

        print(f"[Compression] {len(old_docs)} messages compressed.")

    def add_to_history(self, user_text: str, kro_reply: str):
        timestamp = datetime.now().isoformat()

        docs = [
            Document(page_content=user_text,
                     metadata={"role": "user", "timestamp": timestamp}),
            Document(page_content=kro_reply,
                     metadata={"role": "kro", "timestamp": timestamp})
        ]

        vectorstore.add_documents(docs)
        vectorstore.persist()

    def search_history(self, query: str, k=5):
        try:
            results = vectorstore.similarity_search(query, k=k)
            return [doc.page_content for doc in results]
        except:
            return []

    def step(self, text: str):
        relevant = self.search_history(text, k=5)
        context = "\n".join(relevant)

        prompt = f"""
Контекст памяти:
{context}

Пользователь: {text}

Ответь как KRO.
"""

        reply = llm_generate(prompt)

        self.add_to_history(text, reply)

        if self.should_compress():
            self.compress_conversation()

        return reply


kro = KRO()

# ============================================================
# FastAPI + Gradio
# ============================================================

app = FastAPI(title="KRO v30 API")

@app.get("/")
def health():
    return {"status": "KRO v30 alive"}

gradio_interface = gr.Interface(
    fn=lambda message, history: (history + [(message, kro.step(message))], ""),
    inputs=[gr.Textbox(label="Сообщение"), gr.Chatbot()],
    outputs=[gr.Chatbot(), gr.Textbox(label="Ответ")],
    title="KRO v30 — Постоянная память + Chroma"
)

app = gr.mount_gradio_app(app, gradio_interface, path="/ui")

# ============================================================
# Entry Point
# ============================================================

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 10000))
    print(f"KRO v30 starting on port {port}")
    uvicorn.run(app, host="0.0.0.0", port=port)
